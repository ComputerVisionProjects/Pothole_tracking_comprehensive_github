{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This documentation is a guide and a explanation to all the scripts used in the pothole tracking project. All the code will be explained with the help of comments please pay attention to them.</p>"},{"location":"#directory-structure-of-the-project","title":"Directory structure of the project","text":"<p>The structure of the project is given below</p> <p><pre><code>\u251c\u2500\u2500\u2500.github\n\u2502   \u2514\u2500\u2500\u2500workflows\n\u251c\u2500\u2500\u2500datasets\n\u2502   \u2514\u2500\u2500\u2500coco8\n\u2502       \u251c\u2500\u2500\u2500images\n\u2502       \u2502   \u251c\u2500\u2500\u2500train\n\u2502       \u2502   \u2514\u2500\u2500\u2500val\n\u2502       \u2514\u2500\u2500\u2500labels\n\u2502           \u251c\u2500\u2500\u2500train\n\u2502           \u2514\u2500\u2500\u2500val\n\u251c\u2500\u2500\u2500docs\n\u251c\u2500\u2500\u2500Object_detection\n\u2502   \u2514\u2500\u2500\u2500build\n\u2502       \u251c\u2500\u2500\u2500CMakeFiles\n\u2502       \u2502   \u251c\u2500\u2500\u25003.30.0-rc4\n\u2502       \u2502   \u2502   \u251c\u2500\u2500\u2500CompilerIdC\n\u2502       \u2502   \u2502   \u2502   \u2514\u2500\u2500\u2500Debug\n\u2502       \u2502   \u2502   \u2502       \u2514\u2500\u2500\u2500CompilerIdC.tlog\n\u2502       \u2502   \u2502   \u251c\u2500\u2500\u2500CompilerIdCXX\n\u2502       \u2502   \u2502   \u2502   \u2514\u2500\u2500\u2500Debug\n\u2502       \u2502   \u2502   \u2502       \u2514\u2500\u2500\u2500CompilerIdCXX.tlog\n\u2502       \u2502   \u2502   \u2514\u2500\u2500\u2500VCTargetsPath\n\u2502       \u2502   \u2502       \u2514\u2500\u2500\u2500x64\n\u2502       \u2502   \u2502           \u2514\u2500\u2500\u2500Debug\n\u2502       \u2502   \u2502               \u2514\u2500\u2500\u2500VCTargetsPath.tlog\n\u2502       \u2502   \u2514\u2500\u2500\u250084b16d11d6c0300ba713afeda4afe298\n\u2502       \u251c\u2500\u2500\u2500Debug\n\u2502       \u251c\u2500\u2500\u2500Object_detection.dir\n\u2502       \u2502   \u2514\u2500\u2500\u2500Debug\n\u2502       \u2502       \u2514\u2500\u2500\u2500Object_detection.tlog\n\u2502       \u2514\u2500\u2500\u2500x64\n\u2502           \u2514\u2500\u2500\u2500Debug\n\u2502               \u251c\u2500\u2500\u2500ALL_BUILD\n\u2502               \u2502   \u2514\u2500\u2500\u2500ALL_BUILD.tlog\n\u2502               \u2514\u2500\u2500\u2500ZERO_CHECK\n\u2502                   \u2514\u2500\u2500\u2500ZERO_CHECK.tlog\n\u251c\u2500\u2500\u2500runs\n\u2502   \u2514\u2500\u2500\u2500detect\n\u2502       \u2514\u2500\u2500\u2500train\n\u2502           \u2514\u2500\u2500\u2500weights\n\u251c\u2500\u2500\u2500scripts\n\u2502   \u251c\u2500\u2500\u2500Camera_calibration_images\n\u2502   \u251c\u2500\u2500\u2500models\n\u2502   \u2502   \u2514\u2500\u2500\u2500best(1)_openvino_model\n\u2502   \u251c\u2500\u2500\u2500yolov8_test\n\u2502   \u2514\u2500\u2500\u2500__pycache__\n\u251c\u2500\u2500\u2500train\n\u2502   \u251c\u2500\u2500\u2500images\n\u2502   \u2514\u2500\u2500\u2500labels\n\u251c\u2500\u2500\u2500val\n\u2502   \u251c\u2500\u2500\u2500images\n\u2502   \u2514\u2500\u2500\u2500labels\n\u2514\u2500\u2500\u2500YOLOv8\n    \u2514\u2500\u2500\u2500yolov8n\n</code></pre> In this documentation most of our focus will be on what the scripts in the <code>./scripts</code> directory do.</p>"},{"location":"#setting-up-the-project","title":"Setting up the project","text":"<p>To setup the project you mainly need to know conda, yolo/ultralytics. I am attaching a link to the tutorial here. Youtube link: yolo tutorial link for local machine</p>"},{"location":"page1/","title":"Overview of Model Testing Scripts for Lab Evaluation","text":"<p>This section documents the scripts used to evaluate different versions of the object detection models in a controlled lab environment. Each script is listed individually, along with a brief explanation of its role in the testing process. The titles provided correspond directly to the filenames of the scripts for easy reference.</p>"},{"location":"page1/#script-for-executing-the-model-using-the-native-yolo-library","title":"Script for executing the model using the native YOLO library.","text":"<p>yolo_object_detection_YOLOLIB.py<pre><code>import cv2\nfrom ultralytics import YOLO\nimport torch\nimport time\n\n# Set CUDA device\ntorch.cuda.set_device(0)\n\n# Load the YOLOv8 model\nmodel = YOLO(r\"D:\\Arybhatta_motors_computer_vision\\Yolov8_custom\\scripts\\models\\yolov11l_best.pt\")\n\n# Run detection on an image once (optional, just testing)\nimage_path = r\"D:\\Arybhatta_motors_computer_vision\\Yolov8_custom\\scripts\\videos_images\\istockphoto-174662203-612x612.jpg\"\nimage_results = model(image_path)\n\n# Video path\nvideo_path = r\"D:\\Arybhatta_motors_computer_vision\\Yolov8_custom\\scripts\\videos_images\\WhatsApp Video 2024-07-05 at 01.41.50_72d4a5c5.mp4\"\ncap = cv2.VideoCapture(video_path)\n\narr = []  # to store inference times\n\n# Loop through the video frames\nwhile cap.isOpened():\n    success, frame = cap.read()\n    if not success or frame is None:\n        break\n\n    # Resize frame if needed\n    frame = cv2.resize(frame, (640, 640))\n\n    # Run YOLOv8 detection\n    start_time = time.time()\n    results = model(frame)\n    end_time = time.time()\n\n    # FPS calculation\n    inference_time = end_time - start_time\n    arr.append(inference_time)\n    FPS = str(int(1 / inference_time)) if inference_time &gt; 0 else \"0\"\n\n    # Plot results\n    annotated_frame = results[0].plot()\n\n    # Optional: Print box coordinates\n    for box in results[0].boxes:\n        x1, y1, x2, y2 = box.xyxy[0]\n        print(\"Box coordinates:\", x1.item(), y1.item(), x2.item(), y2.item())\n\n    # Draw FPS on frame\n    cv2.putText(annotated_frame, f\"FPS = {FPS}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (178, 255, 102), 2)\n    cv2.imshow(\"YOLOv8 Detection\", annotated_frame)\n\n    # Break on 'q'\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\n# Release everything\ncap.release()\ncv2.destroyAllWindows()\n\n# Print average inference time\nprint(\"Average inference time per frame:\", sum(arr) / len(arr))\n</code></pre> The script above executes a YOLO model\u2014irrespective of its version\u2014and performs inference on each extracted image frame. Frame extraction, whether from a live camera feed or video stream, is handled using OpenCV. Additionally, the script measures the inference time for every frame; taking the inverse of this value provides the frames per second (FPS).</p>"},{"location":"page1/#script-for-executing-the-yolo-model-using-opencvs-dnn-module","title":"Script for executing the YOLO model using OpenCV's DNN module.","text":"<p>yolo_object_detection_OPENCV.py<pre><code>import cv2\nimport numpy as np\n\n# Load the frozen graph\nnet = cv2.dnn.readNet(r\"D:\\Aryabhatta_computer_vision\\Yolov8_custom\\scripts\\models\\saved_model.pb\")\n\n# Specify target device (CPU or GPU)\nnet.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\nnet.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\n\n# Initialize video capture object\ncap = cv2.VideoCapture(r\"E:\\Aryabhatta_motors_computer_vision\\scripts\\videos\\WhatsApp Video 2024-07-05 at 01.41.49_9b652ede.mp4\")\n\n\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Prepare input image (resize if necessary)\n    frame_resized = cv2.resize(frame, (640, 640))\n    blob = cv2.dnn.blobFromImage(frame, 1.0, (640, 640), (0, 0, 0), swapRB=True, crop=False)\n    net.setInput(blob)\n\n    # Perform inference and get output\n    outs = net.forward()\n\n    # Post-process the output (typically, YOLOv3 or similar models)\n    for out in outs:\n        for detection in out:\n            scores = detection[5:]\n            class_id = np.argmax(scores)\n            confidence = scores[class_id]\n            if confidence &gt; 0.99:\n                # Calculate bounding box coordinates\n                center_x = int(detection[0] * frame.shape[1])\n                center_y = int(detection[1] * frame.shape[0])\n                width = int(detection[2] * frame.shape[1])\n                height = int(detection[3] * frame.shape[0])\n                left = int(center_x - width / 2)\n                top = int(center_y - height / 2)\n\n                # Draw bounding box\n                cv2.rectangle(frame, (left, top), (left + width, top + height), (0, 255, 0), 2)\n\n                # Print bounding box coordinates\n                print(f\"Bounding box coordinates: (left={left}, top={top}, right={left+width}, bottom={top+height})\")\n\n    # Display the frame\n    cv2.imshow('Frame', frame)\n\n    # Exit on pressing 'q'\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\n# Release resources\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> This script closely resembles the one mentioned above, but it utilizes a different version of the model. Specifically, the YOLO model is first converted to ONNX format and then to a TensorFlow frozen graph (.pb file). Inference is performed using OpenCV. The purpose of these conversions is to benchmark the model\u2019s performance across various formats to identify the most efficient one.</p>"},{"location":"page1/#code-to-run-the-onnx-format-of-the-model","title":"Code to run the onnx format of the model","text":"<p>yolo_object_detection_onnx.py<pre><code>import cv2\n\nfrom ultralytics import YOLO\n\nimport torch\n\n# torch.cuda.set_device(0)\n\n# Load the YOLOv8 model\nmodel = YOLO(r\"E:\\Aryabhatta_motors_computer_vision\\scripts\\models\\best.onnx\")\nprint(\"before: \",model.device.type)\nresults = model(r\"E:\\Aryabhatta_motors_computer_vision\\images_potholes_1\\dataset-pothole\\yolov8_custom\\train\\images\\01_jpg.rf.3ca97922642224c05e3602b324e899f2.jpg\")\n#Open the video file\nprint(\"after: \",model.device.type)\n# The inference on the image is done to utilize the gpu for the inference run on the video, this is a bug in yolo and this method helps us bypass the issue.\nvideo_path = r\"E:\\Aryabhatta_motors_computer_vision\\scripts\\videos\\WhatsApp Video 2024-07-05 at 01.41.50_72d4a5c5 (online-video-cutter.com).mp4\"\ncap = cv2.VideoCapture(video_path)\n\n# Loop through the video frames\nwhile cap.isOpened():\n    # Read a frame from the video\n    success, frame = cap.read()\n\n\n    if success:\n        # Run YOLOv8 tracking on the frame, persisting tracks between frames\n        results = model.track(frame, persist=True)\n        for result in results:\n            boxes = result.boxes\n            for box in boxes:\n                x1, y1, x2, y2 = box.xyxy[0]  # Get coordinates in format [x1, y1, x2, y2]\n                print(\"Box coordinates:\", x1, y1, x2, y2)\n        # Visualize the results on the frame\n        annotated_frame = results[0].plot()\n        print(annotated_frame)\n        # Display the annotated frame\n        cv2.imshow(\"YOLOv8 Tracking\", annotated_frame)\n\n        # Break the loop if 'q' is pressed\n        if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n            break\n    else:\n        # Break the loop if the end of the video is reached\n        break\n\n# Release the video capture object and close the display window\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> The above code runs inference using the onnx model, for all the reasons mentioned above.</p>"},{"location":"page1/#script-for-executing-an-ssd-model-in-tflite-format-using-the-mediapipe-framework","title":"Script for executing an SSD model in TFLite format using the Mediapipe framework.","text":"<p>Mediapipe is a framework capable of running SSD models. While SSD offers faster performance compared to YOLO, it falls significantly short in terms of accuracy. Use this script if you need to evaluate SSD models. Note that all model files can be found in the /scripts/models directory. tflite_mediapipe_detections.py<pre><code>import cv2\nimport numpy as np\nimport mediapipe as mp\nimport time\n\n# Load MediaPipe Object Detector\nBaseOptions = mp.tasks.BaseOptions\nVisionRunningMode = mp.tasks.vision.RunningMode\nObjectDetector = mp.tasks.vision.ObjectDetector\nObjectDetectorOptions = mp.tasks.vision.ObjectDetectorOptions\n\n# Path to your TFLite model\nmodel_path = r\"D:\\Aryabhatta_computer_vision\\Yolov8_custom\\scripts\\models\\best3.tflite\"\n\n# Create Object Detector Options\noptions = ObjectDetectorOptions(\n    base_options=BaseOptions(model_asset_path=model_path),\n    running_mode=VisionRunningMode.IMAGE,\n    max_results=5,  # Max number of objects per frame\n    score_threshold=0.2  # Confidence threshold\n)\n\n# Open video file\ncap = cv2.VideoCapture(1)\n\ntl = [427, 381]\ntr = [752,381]\nbl = [161,588]\nbr = [1029,588]\n\n# Initialize the detector   \nwith ObjectDetector.create_from_options(options) as detector:\n    prev_time = time.time()  # Start time for FPS calculation\n    while cap.isOpened():\n        ret, frame = cap.read()\n        # frame = cv2.flip(frame, 1)\n        if not ret:\n            break  # End of video\n        cv2.circle(frame, tl, 5, (0,0,255,-1))\n        cv2.circle(frame, bl, 5, (0,0,255,-1))\n        cv2.circle(frame, tr, 5, (0,0,255,-1))\n        cv2.circle(frame, br, 5, (0,0,255,-1))\n        # cv2.circle(frame, (373, 316), 5, (0,0,255,-1))\n\n        pts1 = np.float32([tl, bl, tr, br])\n        pts2 = np.float32([[0,0], [0,480], [640,0], [640,480]])\n\n        matrix = cv2.getPerspectiveTransform(pts1, pts2)\n        result_0 = cv2.warpPerspective(frame, matrix, (640,480))\n\n        # Convert frame to RGB (MediaPipe format)\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n\n        # Run object detection\n        model_interpreter_start_time = time.time()\n        result = detector.detect(mp_image)\n        each_interpreter_time = time.time() - model_interpreter_start_time\n\n        # Calculate FPS\n        curr_time = time.time()\n        fps = int(1 / (curr_time - prev_time))\n        prev_time = curr_time\n\n        # Draw results\n        if result.detections:\n            for detection in result.detections:\n                bbox = detection.bounding_box\n                x_min, y_min = int(bbox.origin_x), int(bbox.origin_y)\n                width, height = int(bbox.width), int(bbox.height)\n                x_max, y_max = x_min + width, y_min + height\n                print((x_max - x_min)*(y_max - y_min))\n                if (x_max - x_min)*(y_max - y_min) &lt; 250000:\n                    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n                    # Draw label &amp; score\n                    for category in detection.categories:\n                        label = category.category_name\n                        score = category.score\n                        cv2.putText(result_0, f\"{label}: {score:.2f}\", (x_min, y_min - 10),\n                                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n        # Draw FPS on the frame\n        cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n        # Show processed video\n        cv2.imshow(\"Pre_transform\", frame)\n        cv2.imshow(\"Object Detection\", result_0)\n\n        # Press 'q' to exit\n        if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n</code></pre></p>"},{"location":"page2/","title":"Script for exporting the model into various formats.","text":"<p>The following section outlines the code used to export the trained model into various formats. By default, training a YOLO model produces a <code>.pt</code> file, which is the native PyTorch format.</p>"},{"location":"page2/#script-for-converting-the-model-to-the-onnx-format","title":"Script for converting the model to the ONNX format.","text":"onnx_converter.py<pre><code>from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO(r\"yolov8n.pt\")  # load an official model\nmodel = YOLO(r\"E:\\Aryabhatta_motors_computer_vision\\scripts\\models\\best.pt\")  # load a custom trained model\n\n# Export the model\nmodel.export(format=\"onnx\")\n</code></pre> <p>This code uses the native yolo library called ultralytics to export the yolo model into onnx.</p>"},{"location":"page2/#code-used-to-export-the-yolo-model-to-openvino","title":"Code used to export the yolo model to openvino","text":"yolo_openvino_export.py<pre><code>from ultralytics import YOLO  # For YOLOv5 and YOLOv8\n\nmodel = YOLO(r\"D:\\Aryabhatta_computer_vision\\Yolov8_custom\\scripts\\models\\best(1).pt\")\n\nmodel.export(format=\"openvino\")\n</code></pre> <p>The above script is used to convert the YOLO model into the OpenVINO format, which is a quantized version of the model.</p>"},{"location":"page2/#script-for-exporting-the-model-to-the-tensorrt-format","title":"Script for exporting the model to the TensorRT format.","text":"<p>TensorRT is an optimization library designed to enhance inference speed. While it offers several other benefits, I won't go into those details here.</p> yolov8_tensorrt.py<pre><code>from ultralytics import YOLO\n\nmodel = YOLO(r\"D:\\Aryabhatta\\Yolov8_custom\\scripts\\models\\best.pt\")\nmodel.export(\n    format=\"engine\")\n</code></pre>"},{"location":"page3/","title":"Script used for camera calibration","text":"<p>Although the primary goal of this documentation is to explain the code and its functionality, I will briefly cover the concept of camera calibration and its uses. Camera calibration is performed to determine the intrinsic matrix, distortion matrix, and several other parameters. The intrinsic matrix is essential for calculating the homography matrix, which, in turn, enables the conversion of pixel coordinates into real-world coordinates.</p> Camera_calibration.py<pre><code>import cv2\nimport numpy as np\nimport os\nimport glob\n\n# Defining the dimensions of checkerboard\nCHECKERBOARD = (6,8)\ncriteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)\n\n# Creating vector to store vectors of 3D points for each checkerboard image\nobjpoints = []\n# Creating vector to store vectors of 2D points for each checkerboard image\nimgpoints = [] \n\n\n# Defining the world coordinates for 3D points\nobjp = np.zeros((1, CHECKERBOARD[0] * CHECKERBOARD[1], 3), np.float32)\nobjp[0,:,:2] = np.mgrid[0:CHECKERBOARD[0], 0:CHECKERBOARD[1]].T.reshape(-1, 2)\nprev_img_shape = None\n\n# Extracting path of individual image stored in a given directory\nimages = glob.glob(r'D:\\Aryabhatta_computer_vision\\Yolov8_custom\\scripts\\Camera_calibration_images\\*.jpg')\nfor fname in images:\n    img = cv2.imread(fname)\n    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n    # Find the chess board corners\n    # If desired number of corners are found in the image then ret = true\n    ret, corners = cv2.findChessboardCorners(gray, CHECKERBOARD, cv2.CALIB_CB_ADAPTIVE_THRESH + cv2.CALIB_CB_FAST_CHECK + cv2.CALIB_CB_NORMALIZE_IMAGE)\n\n    \"\"\"\n    If desired number of corner are detected,\n    we refine the pixel coordinates and display \n    them on the images of checker board\n    \"\"\"\n    if ret == True:\n        objpoints.append(objp)\n        # refining pixel coordinates for given 2d points.\n        corners2 = cv2.cornerSubPix(gray, corners, (11,11),(-1,-1), criteria)\n\n        imgpoints.append(corners2)\n\n        # Draw and display the corners\n        img = cv2.drawChessboardCorners(img, CHECKERBOARD, corners2, ret)\n\n    cv2.imshow('img',img)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n            break\n\n\ncv2.destroyAllWindows()\n\nh,w = img.shape[:2]\n\n\"\"\"\nPerforming camera calibration by \npassing the value of known 3D points (objpoints)\nand corresponding pixel coordinates of the \ndetected corners (imgpoints)\n\"\"\"\nret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n\nprint(\"Camera matrix : \\n\") \nprint(mtx) # mtx is the intrinsic matrix\nprint(\"dist : \\n\")\nprint(dist) # dist is the distortion matrix\nprint(\"rvecs : \\n\")\nprint(rvecs) # rotation matrix for the images\nprint(\"tvecs : \\n\")\nprint(tvecs) # translation matrix for the images\n</code></pre>"},{"location":"page4/","title":"Script used to develop and evaluate the tracking system and mechanism","text":"<p>This section details the code used for the tracking mechanism in a lab setting. It's important to note that there are two approaches for implementing the tracking system: one involves mounting the camera, while the other involves fixing the camera in place.</p>"},{"location":"page4/#code-used-to-implement-the-solution-with-a-mounted-camera","title":"Code used to implement the solution with a mounted camera","text":"<p>The below code is used to implement the mounted camera method, it uses a tensorflow ssd model to run inference.</p> <p>tensorflow_ssd_object_detection_OPENCV.py<pre><code># --- Import necessary libraries ---\nimport os\nimport cv2\nimport numpy as np\nimport sys\nimport glob\nimport random\nimport importlib.util\nfrom tensorflow.lite.python.interpreter import Interpreter\nimport matplotlib.pyplot as plt\nimport time\n\n# --- Import PyFirmata for Arduino control ---\nimport pyfirmata\nfrom pyfirmata import Arduino, SERVO, util\nfrom time import sleep\n\n# --- Set up Arduino board and define servo pins ---\nport = 'COM6'\npin = 10   # Horizontal movement servo\npin1 = 9   # Vertical movement servo\nboard = pyfirmata.Arduino(port)\nboard.digital[pin].mode = SERVO\nboard.digital[pin1].mode = SERVO\n\n# --- Function to rotate a servo motor connected to the given pin ---\ndef rotate_servo(pin, angle):\n    board.digital[pin].write(angle)\n\n# Initialize servo angles\nangle = 45 \nangle1 = 45\nrotate_servo(pin, angle)\nrotate_servo(pin1, angle1)\n\n# --- Load TensorFlow Lite model ---\nmodelpath = r\"D:\\Aryabhatta_computer_vision\\Yolov8_custom\\scripts\\models\\detect1.tflite\"\ninterpreter = Interpreter(model_path=modelpath)\ninterpreter.allocate_tensors()\nmin_conf = 0.55  # Minimum confidence threshold\narr = []  # Store inference times\n\n# --- Get model input/output details ---\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\nfloat_input = (input_details[0]['dtype'] == np.float32)\ninput_mean = 127.5\ninput_std = 127.5\n\n# --- Load label map ---\nwith open(\"labelmap.txt\", \"r\") as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# --- Start capturing video ---\nvideo_path = r\"D:\\Aryabhatta_computer_vision\\Yolov8_custom\\scripts\\videos\\WhatsApp Video 2024-07-05 at 01.41.50_72d4a5c5 (online-video-cutter.com).mp4\"\ncap = cv2.VideoCapture(1)  # Use webcam (change to 0 or path if needed)\n\n# --- Main loop for processing video frames ---\nwhile cap.isOpened():\n    success, frame = cap.read()\n    if frame is None:\n        break\n\n    if success:\n        image = cv2.flip(frame, 1)  # Flip frame horizontally for mirror effect\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        imH, imW, _ = image.shape\n        print(image.shape)\n\n        # Resize frame to model's expected input size\n        image_resized = cv2.resize(image_rgb, (width, height))\n        input_data = np.expand_dims(image_resized, axis=0)\n\n        if float_input:\n            input_data = (np.float32(input_data) - input_mean) / input_std\n\n        # --- Run inference ---\n        start_time = time.time()\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        interpreter.invoke()\n        end_time = time.time()\n\n        # Calculate and store inference time\n        total_time = end_time - start_time\n        arr.append(total_time)\n        FPS = str(int(1 / (total_time)))  # Frames per second\n\n        # --- Get model outputs ---\n        boxes = interpreter.get_tensor(output_details[1]['index'])[0]    # Bounding boxes\n        classes = interpreter.get_tensor(output_details[3]['index'])[0]  # Class indices\n        scores = interpreter.get_tensor(output_details[0]['index'])[0]   # Confidence scores\n\n        detections = []\n\n        # --- Loop through detections ---\n        for i in range(len(scores)):\n            if ((scores[i] &gt; min_conf) and (scores[i] &lt;= 1.0)):\n                # Convert bounding box coordinates to image size\n                ymin = int(max(1, (boxes[i][0] * imH)))\n                xmin = int(max(1, (boxes[i][1] * imW)))\n                ymax = int(min(imH, (boxes[i][2] * imH)))\n                xmax = int(min(imW, (boxes[i][3] * imW)))\n                x_center = (xmin + xmax) / 2\n                y_center = (ymin + ymax) / 2\n\n                print(x_center, y_center)\n\n                # Draw bounding box\n                cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n\n                # Deadband region is defined\n                if (x_center &gt; 300 and x_center &lt; 420) and (y_center &gt; 200 and y_center &lt; 320):\n                    continue\n\n                # Servo logic: move left/right based on x position\n                if x_center &lt; 320:\n                    angle = angle - 1\n                    rotate_servo(pin, angle)\n                if x_center &gt; 320:\n                    angle = angle + 1\n                    rotate_servo(pin, angle)\n\n                # Servo logic: move up/down based on y position\n                if y_center &lt; 240:\n                    angle1 = angle1 - 1\n                    rotate_servo(pin1, angle1)\n                if y_center &gt; 240:\n                    angle1 = angle1 + 1\n                    rotate_servo(pin1, angle1)\n\n                # Prepare and draw label\n                object_name = labels[int(classes[i])]\n                label = '%s: %d%%' % (object_name, int(scores[i] * 100))\n                labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n                label_ymin = max(ymin, labelSize[1] + 10)\n                cv2.rectangle(image, (xmin, label_ymin - labelSize[1] - 10),\n                              (xmin + labelSize[0], label_ymin + baseLine - 10), (255, 255, 255), cv2.FILLED)\n                cv2.putText(image, label, (xmin, label_ymin - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n                # Append detection details\n                detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n\n        # Display FPS on frame\n        cv2.putText(image, f\"FPS = {FPS}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (178, 255, 102), 2)\n\n        # Show the annotated frame\n        cv2.imshow('annotated frame', image)\n\n        # Press 'q' to quit\n        if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n            break\n\n# --- Cleanup ---\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> The code above adjusts the position of the mechanism by comparing the center of the bounding box of the pothole with the center of the camera. It corrects the position by calculating the error in each iteration and gradually reducing it. However, the issue with this approach is that the algorithm may cause the mechanism to overshoot its target.</p>"},{"location":"page4/#code-used-to-implement-the-stationary-camera-solution","title":"Code used to implement the stationary camera solution","text":"<p>yolo_final_implementation_optimized.py<pre><code>import cv2\nimport numpy as np\nfrom ultralytics import YOLO\nfrom Equation_solver_closed_form import tangent_point_finder\nimport math\nimport time\nfrom esp32_wifi import send_servo_angles\nimport torch\nfrom threading import Thread\n\ndef send_angles_async(a1, a2):\n    Thread(target=send_servo_angles, kwargs={'s1': a1, 's2': a2}, daemon=True).start()\n\nKNOWN_DISTANCE = 50\nKNOWN_WIDTH = 14.5\nface_width_in_frame = 238\n\n# Perspective transform corners (only set once)\ntl = [336, 434]\ntr = [862, 427]\nbl = [124, 578]\nbr = [1143, 559]\npts1 = np.float32([tl, bl, tr, br])\npts2 = np.float32([[0, 0], [0, 720], [1280, 0], [1280, 720]])\nmatrix = cv2.getPerspectiveTransform(pts1, pts2)\n\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter('output_transformed.avi', fourcc, 20.0, (1280, 720))\n\n# Constants\nH = 90\nscale_factor_x = 280 / 1280\nscale_factor_y = 240 / 720\nhlf_width_pprgn = 140\nvertical_width_pprgn = 240\n\ntorch.cuda.set_device(0)\nmodel = YOLO(\"D:\\Arybhatta_motors_computer_vision\\Pothole-tracking-comprehensive\\scripts\\models\\yolov11n_best_25k.pt\")\n\ncam = cv2.VideoCapture(2)\ncam.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\ncam.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\ncam.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n\nprev_frame_time = 0\n\n\ndef send_angles_async(a1, a2):\n    Thread(target=send_servo_angles, kwargs={'s1': a1, 's2': a2}, daemon=True).start()\n\nif not cam.isOpened():\n    print(\"Error: Could not load camera\")\n    exit()\n\nwhile cam.isOpened():\n    ret, frame = cam.read()\n    if not ret:\n        continue\n\n    frame = cv2.flip(frame, 0)\n    for pt in [tl, bl, tr, br]:\n        cv2.circle(frame, pt, 5, (0, 0, 255), -1)\n\n\n    frame = cv2.flip(frame, 1)\n    image_resized = frame\n\n    # Perspective transform\n    warped = cv2.warpPerspective(image_resized, matrix, (1280, 720))\n\n\n    # Inference\n    start_infer = time.time()\n    results = list(model.predict(warped, conf=0.2, device=\"cuda\", stream=True, verbose=False))\n    print(\"Inference time:\", time.time() - start_infer)\n\n    if not results:\n        cv2.imshow(\"YOLOv8 Stream\", warped)\n        if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            break\n        continue\n\n    result = results[0]\n    boxes = result.boxes\n\n    if boxes is not None and len(boxes) &gt; 0:\n        y_centers, rect_sizes, coords = [], [], []\n\n        for box in boxes:\n            coords_np = box.xyxy[0].detach().cpu().numpy()\n            if coords_np.shape[0] != 4:\n                continue\n            x1, y1, x2, y2 = coords_np\n            x_center = float((x1 + x2) / 2)\n            y_center = float((y1 + y2) / 2)\n            area = (x2 - x1) * (y2 - y1)\n\n            y_centers.append(y_center)\n            rect_sizes.append(area)\n            coords.append([x1, y1, x2, y2])\n\n        if coords:\n            coords = np.array(coords)\n            y_arr = np.array(y_centers)\n            size_arr = np.array(rect_sizes)\n            weights = (y_arr / 480) * 0.2 + (size_arr / (640 * 480)) * 0.8\n            idx = np.argmax(weights)\n\n            x1, y1, x2, y2 = coords[idx]\n            x_center = (x1 + x2) / 2\n            y_center = (y1 + y2) / 2\n            warped = cv2.rectangle(warped, (int(x1), int(y1)), (int(x2), int(y2)), (255, 248, 150), 4)\n\n            # FPS calc\n            new_frame_time = time.time()\n            fps = 1 / (new_frame_time - prev_frame_time + 1e-8)\n            prev_frame_time = new_frame_time\n            cv2.putText(warped, f\"FPS: {int(fps)}\", (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n            # Angle calculations\n            x_coord = scale_factor_x * x_center\n            y_coord = scale_factor_y * y_center\n            x_trans = x_coord - hlf_width_pprgn\n            y_trans = 250 + (vertical_width_pprgn - y_coord)\n            ox = x_trans + 6.5\n            oy = y_trans + 1\n\n            ang1 = math.degrees(math.atan(H / math.sqrt(ox ** 2 + oy ** 2)))\n            ang2 = math.degrees(math.atan(ox / oy))\n\n            motor1 = 90 - ang1\n            motor2 = 90 - ang2 + 3\n\n            try:\n                send_angles_async(motor2, motor1)\n                print(f\"Motor1: {round(motor1)}, Motor2: {round(motor2)}\")\n            except Exception as e:\n                print(\"Servo communication failed:\", e)\n\n        cv2.imshow(\"YOLOv8 Stream\", warped)\n        cv2.imshow(\"original\", image_resized)\n    out.write(warped)\n    cv2.imshow(\"YOLOv8 Stream\", warped)\n    cv2.imshow(\"original\", image_resized)\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break    \ncam.release()\ncv2.destroyAllWindows()\n</code></pre> This script implements a real-time pothole detection and servo control system using computer vision and embedded communication. The key components of the pipeline are as follows:</p> <p>Video Acquisition &amp; Perspective Correction: A live video stream is captured from a camera and passed through a homography-based perspective transform to simulate a top-down orthographic view. This improves spatial accuracy for downstream calculations.</p> <p>Object Detection Using YOLO: A YOLOv8 model, pre-trained and optimized for pothole detection, performs inference on each frame using GPU acceleration. Detection confidence is thresholded at 0.2 to filter out low-confidence predictions.</p> <p>Target Prioritization: Among multiple detected bounding boxes, a weighted scoring strategy is applied\u2014favoring larger and lower-positioned boxes (indicative of proximity)\u2014to select the most relevant target for actuation.</p> <p>Kinematic Angle Computation: The pixel coordinates of the selected target are mapped to a scaled coordinate space. From these, two servo angles are computed using basic trigonometric relationships to determine the orientation required to align with the detected object.</p> <p>Asynchronous Servo Communication via ESP32: The computed angles are sent to an ESP32 microcontroller over WiFi using a non-blocking thread. This ensures real-time responsiveness without interrupting the main inference and visualization loop.</p> <p>Visualization &amp; Logging: The processed video frames\u2014annotated with bounding boxes, FPS, and servo data\u2014are displayed in real time and optionally saved to disk for post-analysis.</p> <p>This architecture enables responsive and spatially accurate pothole tracking suitable for autonomous or semi-autonomous robotic systems.</p> <p>Keep in mind I observed even when the camera/apparatus was tilted the error introduced was not significant enough to offset the light path from the pothole, I would suggest anyone working on this project to work on the fixed camera solution only.</p>"},{"location":"page4/#pid-control-system-for-the-mounted-camera-setup","title":"PID control system for the mounted camera setup","text":"<p>PID is a control method that allows us to manage various aspects of motor motion. Below is the code for implementing the PID control system.</p> PID_camera_tracking.py<pre><code># Import required libraries\nimport cv2\nimport numpy as np\nimport time\nimport serial\nfrom simple_pid import PID\nfrom ultralytics import YOLO  # YOLO from Ultralytics\nimport torch\n\n# Select the CUDA device (GPU)\ntorch.cuda.set_device(0)\n\n# Load YOLOv11n model - replace with your trained model path\nmodel = YOLO(r\"D:\\Aryabhatta_computer_vision\\Yolov8_custom\\scripts\\models\\best-yolov11n.pt\")\nprint(\"before: \", model.device.type)\n\n# Test run on a sample image to ensure the model loads correctly\nresults = model(r\"E:\\Aryabhatta_motors_computer_vision\\images_potholes\\78778.png\")\nprint(\"after: \", model.device.type)\n\n# Setup serial communication with Arduino (adjust COM port accordingly)\narduino = serial.Serial(port='COM7', baudrate=9600, timeout=1)\ntime.sleep(2)  # Give some time to establish the serial connection\n\n# Initialize PID controllers for X and Y servos (horizontal and vertical)\npid_x = PID(0.02, 0, 0, setpoint=0)\npid_y = PID(0.02, 0, 0, setpoint=0)\n\n# Set servo angle limits\npid_x.output_limits = (-45, 45)\npid_y.output_limits = (-45, 45)\n\n# Deadband (tolerance) in pixels - no correction if within this range\ndeadband_x = 10\ndeadband_y = 15\n\n# Start video capture (adjust the index based on camera setup)\ncap = cv2.VideoCapture(1)\n\n# Initial angles for the servo motors\nservo_angle_x = 90  # Mid position for horizontal\nservo_angle_y = 40  # Some default vertical position\n\ndef send_servo_command(servo, angle):\n    \"\"\" Send servo angle command to Arduino over serial \"\"\"\n    command = f\"{servo}:{int(angle)}\\n\"\n    arduino.write(command.encode())\n    time.sleep(0.02)  # Small delay for serial communication stability\n\n# Send the initial servo position\nsend_servo_command(\"X\", servo_angle_x)\nsend_servo_command(\"Y\", servo_angle_y)\n\ntry:\n    while True:\n        # Read a frame from the camera and horizontally flip it\n        ret, frame = cap.read()\n        frame = cv2.flip(frame, 1)\n        if not ret:\n            break\n\n        # Run tracking with confidence threshold\n        results = model.track(frame, persist=True, conf=0.5)\n\n        # Set box color for annotation\n        color = (255, 248, 150)\n\n        for result in results:\n            boxes = result.boxes\n            for box in boxes:\n                # Extract bounding box coordinates\n                x1, y1, x2, y2 = box.xyxy[0]\n                object_center_x = (x1 + x2) / 2\n                object_center_y = (y1 + y2) / 2\n\n                # Calculate center of the frame\n                frame_center_x = frame.shape[1] // 2\n                frame_center_y = frame.shape[0] // 2\n\n                # Compute X and Y errors from center\n                error_x = object_center_x - frame_center_x\n                error_y = object_center_y - frame_center_y\n\n                # If horizontal error exceeds tolerance, correct with PID\n                if abs(error_x) &gt; deadband_x:\n                    error_x = error_x.cpu().numpy()\n                    correction_x = pid_x(error_x)\n                    servo_angle_x = np.clip(servo_angle_x - correction_x, 0, 180)\n                    print(servo_angle_x)\n                    send_servo_command(\"X\", servo_angle_x)\n\n                # If vertical error exceeds tolerance, correct with PID\n                if abs(error_y) &gt; deadband_y:\n                    error_y = error_y.cpu().numpy()\n                    correction_y = pid_y(error_y)\n                    servo_angle_y = np.clip(servo_angle_y - correction_y, 0, 60)\n                    print(servo_angle_y)\n                    send_servo_command(\"Y\", servo_angle_y)\n\n        # Display the annotated output\n        annotated_frame = results[0].plot()\n        cv2.imshow(\"Tracking\", annotated_frame)\n\n        # Exit on pressing 'q'\n        if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            break\n\n# On exit, release camera and serial resources\nfinally:\n    cap.release()\n    cv2.destroyAllWindows()\n    arduino.close()\n</code></pre> <p>The deadband refers to the range where the mechanism won't move further if it's pointing within it. PID is a system aimed at minimizing error and determining the best way to reduce it, considering factors like speed, smoothness, and stability. The P (proportional) component controls how quickly the error is reduced, the I (integral) component addresses accumulated error over time, enhancing pothole detection accuracy, and the D (derivative) component stabilizes the motion, preventing overshooting. In our system, a high P value is necessary to match the speed at which the bike moves.</p>"},{"location":"page5/","title":"Code used to implement and evaluate the perspective transformation","text":"<p>In this section, I will explain the code used to detect and test the perspective transformation.</p>"},{"location":"page5/#code-used-to-determine-the-coordinates-of-the-corners-of-the-flat-surface","title":"Code used to determine the coordinates of the corners of the flat surface.","text":"<p>The following code is used to identify the corner coordinates of a flat rectangular surface. This step is crucial for accurately applying the perspective transform to the surface when calculating the pothole coordinates. The code also includes sections for applying the perspective transform, but these can be disregarded here, as the code can also be used to verify the perspective transform.</p> <p>Perspective_transform_coord_finder.py<pre><code>import cv2 \nimport numpy as np \n\n# === Turn on Laptop's webcam ===\n# Index '3' selects the specific webcam device; change if needed (0 is usually default).\ncap = cv2.VideoCapture(3)\n\n# === Define four corner points in the original camera frame for perspective transformation ===\n# These are manually chosen and represent a quadrilateral area that will be transformed to a rectangle.\ntl = [224, 68]     # Top-left corner\nbl = [5, 223]      # Bottom-left corner\ntr = [638, 149]    # Top-right corner\nbr = [565, 420]    # Bottom-right corner\n\n# === Callback function to capture mouse clicks ===\n# Prints the pixel coordinates where the user clicks on the image window.\ndef click_event(event, x, y, flags, param):\n    if event == cv2.EVENT_LBUTTONDOWN:\n        print(f\"Pixel position: ({x}, {y})\")\n\n# === Main loop to continuously capture frames from the webcam ===\nwhile True:\n    ret, frame = cap.read()  # Capture a single frame\n\n    # Display the raw frame before any processing\n    cv2.imshow('frame', frame)\n\n    # === Draw red circles on the perspective transformation corners for visual reference ===\n    cv2.circle(frame, tl, 5, (0, 0, 255), -1)\n    cv2.circle(frame, bl, 5, (0, 0, 255), -1)\n    cv2.circle(frame, tr, 5, (0, 0, 255), -1)\n    cv2.circle(frame, br, 5, (0, 0, 255), -1)\n\n    # Draw another reference point (optional, might be a target or calibration marker)\n    cv2.circle(frame, (373, 316), 5, (0, 0, 255), -1)\n\n    # === Prepare points for perspective transformation ===\n    pts1 = np.float32([tl, bl, tr, br])  # Source points (distorted quadrilateral)\n    pts2 = np.float32([[0, 0], [0, 480], [640, 0], [640, 480]])  # Destination points (rectangle)\n\n    # === Compute the perspective transformation matrix ===\n    matrix = cv2.getPerspectiveTransform(pts1, pts2)\n\n    # === Apply the perspective warp using the matrix ===\n    result = cv2.warpPerspective(frame, matrix, (640, 480))\n\n    # === Show the frame after transformation ===\n    # `frame1` is still showing the original frame here due to the line below,\n    # likely a typo \u2014 should be `cv2.imshow('frame1', result)` if intent is to show transformed image.\n    cv2.imshow('frame1', frame) \n\n    # Enable mouse click tracking on the original frame window\n    cv2.setMouseCallback(\"frame\", click_event)\n\n    # Break the loop if 'q' is pressed\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\n# === Release camera and close all OpenCV windows ===\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> The code above captures mouse clicks and displays the pixel coordinates of the clicked locations. This allows us to easily identify the pixel coordinates of the corners by simply clicking on them in the camera image.</p>"},{"location":"page5/#code-used-to-apply-a-dynamic-perspective-transformation","title":"Code used to apply a dynamic perspective transformation","text":"<p>This code establishes a system that identifies the four corners of a rectangular area and applies a perspective transformation to it.</p> Dynamic_perspective_transform_rect.py<pre><code>import cv2\nimport numpy as np\n\n# Initialize the camera stream\ncap = cv2.VideoCapture(0)\n\n# Edge case handling\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n\n# Loop to read the camera stream\nwhile cap.isOpened():\n    ret, image = cap.read()\n    if not ret:\n        break\n\n    # Convert to HSV\n    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    # Define black color range (adjust as needed)\n    lower_black = np.array([0, 0, 0])\n    upper_black = np.array([180, 255, 50])\n\n    # Mask for black regions (detecting tape)\n    mask = cv2.inRange(hsv, lower_black, upper_black)\n\n    # Find contours\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Find the largest contour (assuming it's the tape)\n    if contours:\n        largest_contour = max(contours, key=cv2.contourArea)\n\n        # Approximate to a polygon\n        epsilon = 0.02 * cv2.arcLength(largest_contour, True)\n        approx = cv2.approxPolyDP(largest_contour, epsilon, True)\n\n        # Ensure we have exactly 4 corners\n        if len(approx) == 4:\n            corners = approx.reshape(4, 2)  # Reshape to (4,2) array\n            print(\"Detected corners:\", corners)\n\n            # Draw the corners on the image\n            for (x, y) in corners:\n                cv2.circle(image, (x, y), 5, (0, 255, 0), -1)\n\n        else:\n            print(f\"Detected {len(approx)} corners, refining...\")\n\n    cv2.imshow(\"Detected Corners\", image)\n    cv2.waitKey(0)\n    cv2.destroyAllWindows()\n</code></pre> <p>The code above is used to identify a rectangular area marked by black tapes. The region is then detected using contour detection, which was done for specific technical reasons.</p>"},{"location":"page6/","title":"Code for utilizing sensors during testing","text":"<p>In this section, I will explain the code used to test various sensors. While these sensors are typically operated with ARDUINO code, they can also be interfaced with Python using frameworks such as pyserial and pyfirmata.</p>"},{"location":"page6/#accelerometer-code","title":"Accelerometer code","text":"<p>Accelerometer_position_tracking.py<pre><code>import serial\nimport time\n\n# Serial setup\nser = serial.Serial('COM7', 115200, timeout=1)\ntime.sleep(2)\n\n# Motion Variables\nvelocity_x, velocity_y, velocity_z = 0, 0, 0\ndistance_x, distance_y, distance_z = 0, 0, 0\nlast_time = time.time()\n\nACCEL_THRESHOLD = 0.5  # Ignore small accelerations since the sensor has a lot of noise\nDECAY_FACTOR = 0.98    # This constant helps in controlling the increment of velocity, if this is not done velocity will keep on increasing \n\nwhile True:\n    try:\n        if ser.in_waiting &gt; 0:\n            data = ser.readline().decode('utf-8').strip()\n            if data:\n                # Parse received data: yaw, pitch, roll, accel_x, accel_y, accel_z\n                yaw, pitch, roll, x, y, z = map(float, data.split(\",\"))\n\n                # Time step\n                current_time = time.time()\n                dt = current_time - last_time\n                last_time = current_time\n\n                # Apply acceleration threshold\n                x = x if abs(x) &gt; ACCEL_THRESHOLD else 0\n                y = y if abs(y) &gt; ACCEL_THRESHOLD else 0\n                z = z if abs(z) &gt; ACCEL_THRESHOLD else 0\n\n                # Integrate acceleration to get velocity\n                velocity_x = (velocity_x + x * dt) * DECAY_FACTOR \n                velocity_y = (velocity_y + y * dt) * DECAY_FACTOR\n                velocity_z = (velocity_z + z * dt) * DECAY_FACTOR\n\n                # Integrate velocity to get distance\n                distance_x += velocity_x * dt\n                distance_y += velocity_y * dt\n                distance_z += velocity_z * dt\n\n                # Print Yaw, Pitch, Roll &amp; Distance\n                print(f\"Yaw: {yaw:.2f}\u00b0, Pitch: {pitch:.2f}\u00b0, Roll: {roll:.2f}\u00b0\")\n                print(f\"Distance -&gt; X: {distance_x:.2f} m, Y: {distance_y:.2f} m, Z: {distance_z:.2f} m\\n\")\n                time.sleep(0.5)\n    except Exception as e:\n        print(\"Error:\", e)\n</code></pre> Although this method should theoretically work, it may not perform as expected in real-world situations. I'm including this code in case anyone finds it useful for conducting some tests.</p>"},{"location":"page7/","title":"Code used for debugging","text":"<p>When developing a system like this, encountering errors is common. I will explain the code I used to resolve these issues.</p>"},{"location":"page7/#script-to-find-the-index-of-the-camera","title":"Script to find the index of the camera","text":"<p>When working with multiple cameras, it's crucial to identify the camera index. To do this, we use loops to detect the index where the camera is available.</p> <p>camera_index_finder.py<pre><code>import cv2\n\ncams_test = 500\nfor i in range(0, cams_test):\n    cap = cv2.VideoCapture(i)\n    test, frame = cap.read()\n    if test:\n        print(\"i : \"+str(i)+\" /// result: \"+str(test))\n</code></pre>  The code is self explanatory.</p>"},{"location":"page7/#code-utilized-to-test-the-functionality-of-the-servo-motors","title":"Code utilized to test the functionality of the servo motors.","text":"<p>When setting up the system, it's important to position the servo motors correctly and verify their functionality before running the main code. The following code is used for this purpose.</p> Pyfirmata_test_code_motor_running.py<pre><code>import pyfirmata\nfrom pyfirmata import Arduino, SERVO, util\nfrom time import sleep\nport = 'COM7' #usb pin\npin = 10 #pin which servo is connected to on digital\npin1 = 9\nboard = pyfirmata.Arduino(port)\nboard.digital[pin].mode = SERVO\nboard.digital[pin1].mode = SERVO    \n\ndef rotate_servo(pin,angle):\n     board.digital[pin].write(angle)\n     board.digital[pin1].write(angle)\n     sleep(0.0015)\n\n\nuser_angle_1 = int(input(\"Input user angle1: \"))\nuser_angle_2 = int(input(\"Input user angle2: \"))\n\nrotate_servo(pin, user_angle_1)\nrotate_servo(pin1, user_angle_2)\n</code></pre> <p>The library pyfirmata is used here to rotate the servo motor.</p>"},{"location":"page7/#undistorting-the-camera-video-stream","title":"Undistorting the camera video stream","text":"<p>Cameras often introduce distortion in the image, and to correct this, the image needs to be undistorted. While I didn't utilize this feature in my system, it could be helpful if you need it.</p> <pre><code>import cv2\nimport numpy as np\n\n# Load your camera matrix and distortion coefficients\n# You need to calibrate your camera beforehand to get these values\nmtx = np.array([[892.86025081, 0, 319.72816385],\n                   [0, 894.36852612, 182.8589494 ],\n                      [0, 0, 1]])\ndist = np.array([-2.17954003e-02, 3.78637782e-01, 4.81147467e-04, 1.20099914e-03,\n  -8.01539551e-01])  # Replace with actual values\n\n# Open webcam feed\ncap = cv2.VideoCapture(1)\n\n# Get frame dimensions\nret, frame = cap.read()\nh, w = frame.shape[:2]\n\n# Compute the optimal new camera matrix and undistort maps\nnew_camera_mtx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w, h), 1, (w, h))\nmapx, mapy = cv2.initUndistortRectifyMap(mtx, dist, None, new_camera_mtx, (w, h), 5)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Undistort the frame\n    undistorted_frame = cv2.remap(frame, mapx, mapy, cv2.INTER_LINEAR)\n\n    # Display the result\n    cv2.imshow(\"distorted_frame\", frame)\n    cv2.imshow(\"Undistorted Feed\", undistorted_frame)\n\n    # Press 'q' to exit\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\n# Release resources\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"page7/#script-to-check-if-the-model-is-readable","title":"Script to check if the model is readable","text":"<p>While running code on one of the office laptops, I encountered an issue where the model path was being read as null. Below is the code to check whether the model path is readable.</p> model_path_checker.py<pre><code>import os\n\nfile_path = r\"D:\\Arybhatta_motors_computer_vision\\Yolov8_custom\\scripts\\models\\best-yolov11n.pt\"\nif os.path.exists(file_path):\n    print(\"File exists!\")\nelse:\n    print(\"File does not exist.\")\n</code></pre>"},{"location":"page8/","title":"Code used to implement the math for the system","text":"<p>This section includes the code used to implement the mathematical aspects of the project. I won't delve into the proofs of the mathematics here, as that has been addressed separately.</p>"},{"location":"page8/#code-used-to-implement-the-tangent-plane-and-determine-the-circles-points","title":"Code used to implement the tangent plane and determine the circle's points","text":"Equation_solver_tangent_plane.py<pre><code># importing from sympy library\nfrom sympy import symbols, Eq, solve, I\nimport math\nimport time\n\ndef tangent_point_finder(x_0, y_0, Radius):\n    # defining the symbolic variable 'z'\n    start = time.perf_counter()\n    x = symbols('x')\n\n    # setting up the complex equation z^2 + 1 = 0\n    equation = Eq(x**2*(x_0**2+y_0**2) - 2*Radius**2*x*x_0 + Radius**2*(Radius**2-y_0**2), 0)\n\n    # solving the equation symbolically to find complex solutions\n    solutions = solve(equation, x)\n    if  x_0 &gt; 0:\n        x_t = solutions[0]\n\n    else:\n        x_t = solutions[1]\n\n    y_t = math.sqrt(Radius**2 - x_t**2)\n    end = time.perf_counter()\n    inference_time = end - start\n    print(f\"inference time: {inference_time}\")\n    return [float(x_t), float(y_t)]\n\nx = 30\ny = 40\ntangent_point_finder(x, y, 33.1)\n</code></pre> <p>The above code utilizes a library called sympy, which assists in solving equations through algebraic representations.</p>"},{"location":"page8/#code-for-the-closed-form-solution","title":"Code for the closed-form solution","text":"<p>The following code utilizes a closed-form solution to determine the coordinates.</p> <p>Equation_solver_closed_form.py<pre><code>from math import sqrt\nimport time\nimport random\n\ndef tangent_point_finder(x0, y0, R):\n        start = time.perf_counter()\n        denominator = x0**2 + y0**2\n        if denominator == 0:\n            raise ValueError(\"Denominator cannot be zero (x0 and y0 cannot both be zero).\")\n\n        term_inside_sqrt = R**4 * x0**2 - R**2 * (x0**2 + y0**2) * (R**2 - y0**2)\n\n        if term_inside_sqrt &lt; 0:\n            raise ValueError(\"Negative value under the square root. No real solution.\")\n\n        sqrt_term = sqrt(term_inside_sqrt)\n\n        x1 = (R**2 * x0 + sqrt_term) / denominator\n        x2 = (R**2 * x0 - sqrt_term) / denominator\n\n        if x0 &gt; 0:\n            x_t = x2\n\n        if x0 &lt; 0:\n            x_t = x1\n\n        y_t = sqrt(R**2 - x_t**2)\n\n        end = time.perf_counter()\n        inference_time = end - start\n        print(f\"inference time: {inference_time}\")\n        return [x_t, y_t]\n</code></pre> Using a closed-form solution makes the process of finding coordinates faster.</p>"},{"location":"page9/","title":"Code used to implement visual motion tracking.","text":"<p>Visual odometry is a technique that allows us to track the camera's position relative to its previous location. This was essential for a fully fixed camera solution, which requires tracking the camera's movement. Although I couldn't get it to work, you can give it a try if needed.</p> camera_pos_vo.py<pre><code>import cv2\nimport numpy as np\n\n# Camera intrinsic matrix (K) - Adjust based on your camera calibration\nK = np.array([[892.86025081, 0, 319.72816385],\n                   [0, 894.36852612, 182.8589494 ],\n                      [0, 0, 1]])\n\n# Capture video (adjust index or file path as needed)\ncap = cv2.VideoCapture(1)  # Use \"video.mp4\" instead of 0 for a file\n\n# Initialize ORB detector\norb = cv2.ORB_create(3000)\n\n# Initialize previous frame data\nret, frame_prev = cap.read()\nif not ret:\n    print(\"Error: Could not read first frame.\")\n    cap.release()\n    exit()\n\ngray_prev = cv2.cvtColor(frame_prev, cv2.COLOR_BGR2GRAY)\nkp_prev, des_prev = orb.detectAndCompute(gray_prev, None)\n\n# Camera position in 3D space (initial)\ncamera_position = np.zeros((3, 1))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    kp, des = orb.detectAndCompute(gray, None)\n\n    if not kp_prev or not kp or des_prev is None or des is None:\n        print(\"No keypoints detected. Skipping frame.\")\n        frame_prev, kp_prev, des_prev = frame, kp, des\n        continue\n\n    # Match keypoints using Brute Force matcher\n    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n    matches = bf.match(des_prev, des)\n    matches = sorted(matches, key=lambda x: x.distance)\n\n    # Ensure valid matches exist\n    valid_matches = [m for m in matches if 0 &lt;= m.queryIdx &lt; len(kp_prev) and 0 &lt;= m.trainIdx &lt; len(kp)]\n    if len(valid_matches) &lt; 10:\n        print(\"Not enough valid matches. Skipping frame.\")\n        frame_prev, kp_prev, des_prev = frame, kp, des\n        continue\n\n    # Extract matched keypoints\n    pts_prev = np.float32([kp_prev[m.queryIdx].pt for m in valid_matches])\n    pts_curr = np.float32([kp[m.trainIdx].pt for m in valid_matches])\n\n    # Estimate Essential Matrix\n    E, _ = cv2.findEssentialMat(pts_curr, pts_prev, K, method=cv2.RANSAC, threshold=1.0)\n\n    if E is not None and E.shape == (3, 3):\n        # Recover camera movement (R = Rotation, t = Translation)\n        _, R, t, _ = cv2.recoverPose(E, pts_curr, pts_prev, K)\n\n        # Update camera position (Z increases forward)\n        camera_position += R @ t\n\n        # Print camera position\n        print(f\"Camera Position: X={camera_position[0,0]:.2f}, Y={camera_position[1,0]:.2f}, Z={camera_position[2,0]:.2f}\")\n\n    # Draw matches\n    match_img = cv2.drawMatches(frame_prev, kp_prev, frame, kp, valid_matches[:min(50, len(valid_matches))], None)\n    cv2.imshow(\"Visual Odometry\", match_img)\n\n    # Update previous frame data\n    frame_prev, kp_prev, des_prev = frame, kp, des\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"}]}